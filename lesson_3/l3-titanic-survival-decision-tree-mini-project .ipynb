{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 3: Titanic Survival Decision Tree Mini-project \n",
    "\n",
    "\n",
    "## Objectives\n",
    "  - Perform exploratory data analysis on a data set to determine relevant features to include in a model.\n",
    "  - Tidy a data set so that all features will be compatible with [the `sklearn` library](http://scikit-learn.org/stable/).\n",
    "      - Change categorical (object) variables to numeric with either [`pandas.Series.map()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) or [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "      - Impute missing or null values with [label-based indexing (`loc`)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)\n",
    "  - Save cleaned datasets (so that we don't lose our hard preprocessing work!)\n",
    "  - Use the `sklearn` library to build a predictive `DecisionTreeClassifier` model for the Titanic Survival Dataset.\n",
    "  - Compute the accuracy of a model on both the training and validation (testing) data.\n",
    "  - Adjust hyperparameters (e.g. `max_depth`) to see the effects on model accuracy.\n",
    "  \n",
    "## Prerequisites\n",
    "  - You should have the following python packages installed:\n",
    "    - [matplotlib](http://matplotlib.org/index.html)\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [sklearn](http://scikit-learn.org/stable/install.html)\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from one of Nick Hoh's excellent [sessions](https://github.com/nickypie/ConnectIntensive). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sklearn\n",
    "skversion = int(sklearn.__version__[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Titanic passenger data into a pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"titanic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two methods for transforming to numeric features: `pandas.Series.map()` and `pandas.get_dummies()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [pandas.Series.map()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) \n",
    "\n",
    "- Takes a Python dictionary as a parameter or argument.  \n",
    "- Keys of the dictionary are the current entries in the `Series` object.\n",
    "- Values of the dictionary are the new desired entries for the `Series` object. \n",
    "\n",
    "Example: `'Sex'` feature. Map the two genders, `'female'` and `'male'`, to numbers, e.g. 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run the cell below to use `Series.map()` to map the genders `'female'` and `'male'` to 0 and 1, respectively, then display the first few rows of the `DataFrame` object, and the `dtype`, to show that the `'Sex'` feature is now numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['Sex'] = train_df['Sex'].map( {'female': 0, 'male': 1, 0:0, 1:1} )\n",
    "display(train_df.head())\n",
    "display(train_df['Sex'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [pandas.get dummies()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `'Embarked'` feature, which has 3 distinct categories, we use pandas.get dummies() \n",
    "\n",
    "Embarked categories: \n",
    "\n",
    "C: Cherbourg\n",
    "Q: Queenstown\n",
    "S: Southampton \n",
    "\n",
    "- Using dictionary and `Series.map()` for`'Embarked'` feature imparts some ordinality (or ordering) to the feature.\n",
    "\n",
    "- Not logical for Cherbourg < Queenstown < Southampton \n",
    "\n",
    "- With 3 or more distinct but unordered categories, it is better to use [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) \n",
    "\n",
    "- [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)turns the original feature into dummy variables (AKA indicator variables).\n",
    "\n",
    "For more information regarding one-hot encoding, you can check out [this Quora post](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science).\n",
    "\n",
    "\n",
    "**Run** the cell below to see what the dummy variables for the `Series` object `train_df['Embarked']` look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(train_df['Embarked'])\n",
    "display(train_dummies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the features\n",
    "\n",
    "**Run** the cell below to rename the features to the full name of the port of embarkation using the method pandas.DataFrame.rename():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dummies = train_dummies.rename(columns={'C':'Cherbourg','Q':'Queenstown','S':'Southampton'})\n",
    "display(train_dummies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine train_dummies and train_df\n",
    "\n",
    "**Run** the cell below to concatenate (combine) `train_df` and `train_dummies` and print the result. Note, the `if` statement ensures that we don't concatenate the dummies more than once if you run the cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'Cherbourg' not in list(train_df.columns):\n",
    "    train_df = pd.concat([train_df, train_dummies], axis=1)\n",
    "display(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing or null values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run the cell below to compute the mean of the `Series` object `train_df['Age']` and properly impute the missing 'Age' values.\n",
    "\n",
    "- Recall that loc is label-based indexing, while iloc is integer-based indexing. \n",
    "- Our row_indexer is the boolean Series object train_df['Age'].isnull(). \n",
    "- Our column_indexer is simply the label-based index 'Age'.\n",
    "\n",
    "Note that the default behavior for [the method `pandas.Series.mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mean.html) is to skip null values, so setting the parameter `skipna=True` isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_age = train_df['Age'].mean(skipna=True)\n",
    "print(\"mean age: {:.1f}\".format(mean_age))\n",
    "\n",
    "train_df.loc[train_df['Age'].isnull(),'Age'] = mean_age\n",
    "display(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the preprocessed data\n",
    "\n",
    "Run the cell below to write the titanic DataFrame object to a csv file using the method pandas.DataFrame.to_csv(). Note, from the module os.path we import the method isfile, which allows us to check whether the file already has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "titanic_data_clean = \"titanic_data_clean.csv\"\n",
    "if not isfile(titanic_data_clean):\n",
    "    train_df.to_csv(titanic_data_clean)\n",
    "    print(\"Cleaned Titanic dataset saved!\")\n",
    "else:\n",
    "    print(\"Cleaned Titanic dataset already saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"titanic_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `train_test_split`\n",
    "\n",
    "  - **Training set:** A set of examples used for machine learning, that is to fit the parameters (*i.e.*, weights) of the classifier.\n",
    "  - **Validation set:** A set of examples used to tune the hyperparameters (*i.e.*, architecture, not weights) of a classifier, for example to choose the maximum depth of a decision tree, or the number of hidden layers in a neural network.\n",
    "  - **Test set:** A set of examples used only *once*. This assesses the performance (generalization) of the fully-specified classifier (once all hyperparameters have been specified).\n",
    "\n",
    "\n",
    "**Run** the cell below to create the `DataFrame` object `X` and the `Series` object `y` from the desired features from `train_df`. Then use `train_test_split()` with a `random_state` to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Starting with scikit-learn version 0.18, the model_selection module replaces the cross_validation module,\n",
    "# so we should import train_test_split from the appropriate module depending on the version number.\n",
    "if skversion >= 18:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "else:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Make a list of the desired feature names for model building\n",
    "desired_features = ['Pclass', 'Sex', 'Age','SibSp','Parch', 'Cherbourg','Queenstown','Southampton']\n",
    "\n",
    "# X is our pandas DataFrame object with the features from which we will predict the 'Survived' feature.\n",
    "X = pd.DataFrame(train_df[desired_features])\n",
    "\n",
    "# y is our pandas Series object with the 'Survived' feature to be predicted.\n",
    "y = pd.Series(train_df['Survived'])\n",
    "\n",
    "# Split the data into training and validation (test) data sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Take a look at the first few rows of the training features and classes\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "\n",
    "# Verify that the data sets were split 80% training and 20% testing\n",
    "print(\"The original data ({} instances) was split into training ({} instances) and testing ({} instances) data sets\".\\\n",
    "     format(len(X),len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree Classifier\n",
    "For supervised learning problems, the model building `sklearn` workflow is pretty similar, regardless of the type of classifier you'd like to build:\n",
    "  1. **Create** a classifier object.\n",
    "  2. **Train** the classifier on the training data set.\n",
    "  3. **Predict** with the classifier on the validation (test) data set.\n",
    "  4. **Assess** the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "\n",
    "Run the cell below to **create** a Decision Tree Classifier, **train** it on the training data, **predict** class labels for the validation (test) data set, and **assess** the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier and accuracy_score \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. CREATE the classifier object.\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "\n",
    "# 2. TRAIN the classifier object using the method .fit()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation test set using the method .predict()\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test  = clf.predict(X_test)\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "print(\"The model with max_depth of {} has an accuracy of {:.1f}% on the training data, and {:.1f}% on the testing data\".\\\n",
    "      format(1,\\\n",
    "             100.0*accuracy_score(y_pred_train,y_train),\\\n",
    "             100.0*accuracy_score(y_pred_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
